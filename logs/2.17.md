Baseline regression head has been created. Pulling features from ResNet50 stage1.block1.layer2, stage2.block1.layer1, stage3.block1.layer1, stage4.block1.layer1. Using a feature pyramid network (FPN) to resize channels dimensions and create better quality features for regression head processing. FPNs help create high resolution semantically rich features.

There is no point in training a custom feature extractor - it's better to establish a baseline with ResNet50 which has been trained on ImageNet with lots of compute and change the feature extractor to another pretrained extractor after proving ResNet50 yields reasonable results.

Regression head takes in FPN features, resizes dimensions via upsampling and pooling operations for concatenation purposes, concats, performs 3 convolution operations, subsequent global feature aggregation to "flatten" the multidimensional information without explicitly using flatten to create an unneccesary number of features, passes through 2 dense layers, and produces regression outputs.

TODO:
- Train current regression head till it overfits to test baseline representational capacity + see if model is able to learn noisy features
- Increase model capacity via upsampling + more conv layers OR increasing FPN feature count to 256
- Try increasing FPN channel output channel count to 256 instead of 128 to see if increase in feature count improves model performance
- Once overfitting occurs + model has representational capacity to learn noisy features, add in regularization with a starting l2 term of 0.001 and change accordingly based on empirical results
- Submit initial model by 2/20/23