Found the minimum validation loss for current model: 0.392 — after 2 epochs. Training loss gets as low as 0.238 after 20 epochs, so it seems like the representational capacity of the current structure is pretty high.

Wrote a training loop (train2.ipynb) to train full models for both the left and right hemisphere together, but I run out of memory — very weird sicnce we have like 15 gigs of RAM, ResNet seems to be taking up 700 MB. One iteration of the training loop should not take up more than a gigabyte (I did the math lol: (4e-6)*64*((3*224*224)+(64*19004)+(64*20544)) (4e-6) = Size of a fp32 in MB. Fucking something up.

We should get this fixed and submit — shouldn't be too much more work to get *something* submitted in the next couple days.

I've been thinking about what we should try next, and I think the answer is diffusion. I hypothesize it might do very very well. Beyond the fact that it's SOTA on pretty much everything (except language), I think the idea of denoising makes a lot of sense for brain data — we see an image, form big picture details, and eventually slowly fill in the gaps (maybe!). Diffusion models "think" in a similar way, so I think this should be worth a shot. We should condition it right from the beginning with our ResNet features.

TODO:
- Fix the memory issue
- Submit current model
- Refine current model to get a better baseline — or get on diffusion. Need to pick.