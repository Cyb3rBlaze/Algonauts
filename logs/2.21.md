Squashed down target distribution to be a unit gaussian, and added a tanh activation function to our final outputs... doing this somehow 10xed performance. In just one epoch of training, validation loss is down to 0.025. CRAZY!

I do not understand why this makes such a big difference... but it is very fascinating. Emperically, from looking at the target distribution for a few samples, it looks like ours is a little more squashed down â€” perhaps it is still worth adding an auxilary spread deviation function to our loss function.

I think this improvement conclusively tells us that our model has enough representational capcity. We should try to get this in as a submission ASAP.

Steps for that:
- [ ] Train entire model with a similar strategy, instead of just a single ROI
    - I suspect something is going wrong with our memory utilization. We're operating over imagener features and trying to output just a scalar, we shouldn't go off the charts with memory!
    - I'm not even able to up the batch size to 64 on our ROI model. We should look into memory stuff soon
- [ ] Calculate score to see how well we do
- [ ] Submit