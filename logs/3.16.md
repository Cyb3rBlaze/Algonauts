Might have narrowed down the problem - it's so obvious now. Learning rate was too high and I failed to properly weigh the effect of prior neuron updates regularizing the learning of subsequent neurons. A lower learning rate is necessary for proper learning of paramaters without going all over the place and encountering stagnant training.

TODO:
- Overfit with 0.001 learning rate on 200 epochs and see how low train loss can get
- Reduce learning rate to 0.0005 on 200+ epochs and see if train loss converges lower
- Find ideal learning rate, introduce regularization and tune from there