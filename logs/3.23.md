Realized process of minimizing overfit train loss was not the proper way to approach finding optimal learning rate + representational capacity.

Remember:
- MAKE SURE TO TRAIN FPN
- Smaller learning rate when representational capacity increases because loss landscape is more complex/difficult to navigate
- Model should not begin overfitting immediately
- Proving overfitting for representational capacity does not require finding optimal lr - it will change when introducing l2/l1, droupout, or changing representational capacity
- Change only one thing at once to isolate cause and effect, do not be impatient because problem diagnosis will be difficult

TODO:
- Train with 0.0001 lr (larger than 0.00005 because trainable representational capacity is decreasing to fpn + regression head)
- Test out regularization values to optimize validation loss